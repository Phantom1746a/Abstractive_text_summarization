# ğŸ¤– Urdu Text Summarization with Mistral 7B + LoRA + Unsloth

A lightweight, efficient fine-tuning pipeline to summarize **Urdu text** using **Mistral 7B**, optimized with **LoRA** and **Unsloth** for fast 4-bit training on consumer GPUs.

ğŸ¯ **Goal**: Fine-tune Mistral 7B to generate concise summaries of Urdu news articles, blogs, or documents.  
âš¡ **Speed**: 2x faster training with **Unsloth**'s optimized kernels.  
ğŸ’¾ **Efficiency**: Uses only **~15GB VRAM** via 4-bit quantization and LoRA.

---

## ğŸ“· Demo (Example)

**Input (Urdu):**  
> "Ù¾Ø§Ú©Ø³ØªØ§Ù† Ú©ÛŒ Ù…Ø¹ÛŒØ´Øª Ø­Ø§Ù„ÛŒÛ Ø¨Ø±Ø³ÙˆÚº Ù…ÛŒÚº Ú†ÛŒÙ„Ù†Ø¬Ø² Ú©Ø§ Ø´Ú©Ø§Ø± Ø±ÛÛŒ ÛÛ’Û” Ù…ÛÙ†Ú¯Ø§Ø¦ÛŒ Ø¨Ú‘Ú¾ÛŒ Ø§ÙˆØ± Ø±ÙˆÙ¾Û’ Ú©ÛŒ Ù‚Ø¯Ø± Ú©Ù… ÛÙˆØ¦ÛŒÛ” Ø­Ú©ÙˆÙ…Øª Ù†Û’ Ø¢Ø¦ÛŒ Ø§ÛŒÙ… Ø§ÛŒÙ Ø³Û’ Ù‚Ø±Ø¶ Ù„ÛŒØ§Û”"

**Output (Summary):**  
> "Ø­Ú©ÙˆÙ…Øª Ù†Û’ Ù…Ø¹ÛŒØ´Øª Ú©Û’ Ú†ÛŒÙ„Ù†Ø¬Ø² Ú©Û’ Ù¾ÛŒØ´Ù Ù†Ø¸Ø± Ø¢Ø¦ÛŒ Ø§ÛŒÙ… Ø§ÛŒÙ Ø³Û’ Ù‚Ø±Ø¶ Ù„ÛŒØ§Û”"

---

## ğŸ§° Features

- âœ… Fine-tunes **Mistral 7B** for **Urdu summarization**
- ğŸ”§ Uses **LoRA (Low-Rank Adaptation)** for parameter-efficient tuning
- âš¡ Powered by **Unsloth** â€“ 2x faster 4-bit training
- ğŸ“Š Supports **CSV datasets** (easy data loading)
- ğŸ’¬ Instruction-tuned prompts in **Urdu**
- ğŸ“¦ Full training & inference scripts 
- ğŸš€ Ready to deploy with Gradio (optional)

---

